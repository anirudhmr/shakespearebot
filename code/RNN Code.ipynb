{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "# From the keras lstm_text_generation.py Github example\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "def gen_rnn_data(n, filename):\n",
    "    all_text = \"\"\n",
    "    with open(filename) as inp:\n",
    "        for line in inp:\n",
    "            if len(line.split()) > 1:\n",
    "                all_text += line[:-1].lower()\n",
    "    seqs = []\n",
    "    for i in range(40, len(all_text), n):\n",
    "        seqs.append(all_text[i-40:i+1])\n",
    "    chars = sorted(list(set('\\n'.join(seqs))))\n",
    "    char_2_index = dict((c, i) for i, c in enumerate(chars))\n",
    "    index_2_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    return seqs, char_2_index, index_2_char \n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "_, mapping_c2i, mapping_i2c = gen_rnn_data(10, '../data/shakespeare.txt')\n",
    "print(mapping_c2i)\n",
    "\n",
    "def train_model(seq_skip, n_nodes, n_epochs, model_filename):\n",
    "    lines, mapping_c2i, mapping_i2c = gen_rnn_data(seq_skip, '../data/shakespeare.txt')\n",
    "    all_text = '\\n'.join(lines)\n",
    "\n",
    "    seqs = []\n",
    "    for line in lines:\n",
    "        # Replace string with encoding of each character\n",
    "        encoded_seq = [mapping_c2i[char] for char in line]\n",
    "        # Replace list of sequences with list of encoded sequences\n",
    "        seqs.append(encoded_seq)\n",
    "\n",
    "    # Number of distinct characters\n",
    "    num_chars = len(mapping_c2i)\n",
    "    print(num_chars)\n",
    "    # Splitting into input strings and output character\n",
    "    seqs = np.array(seqs)\n",
    "    X, y = seqs[:, :-1], seqs[:, -1]\n",
    "\n",
    "    # one-hot encoding\n",
    "    seqs = [to_categorical(x, num_classes = num_chars) for x in X]\n",
    "    X = np.array(seqs)\n",
    "    y = to_categorical(y, num_classes=num_chars)\n",
    "\n",
    "    # Model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_nodes, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dense(num_chars, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=n_epochs, verbose=1)\n",
    "\n",
    "    model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 38)                7638      \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 2.9275 - acc: 0.1779\n",
      "Epoch 2/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 2.5783 - acc: 0.2771\n",
      "Epoch 3/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 2.3839 - acc: 0.3054\n",
      "Epoch 4/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 2.2858 - acc: 0.3366\n",
      "Epoch 5/30\n",
      "9210/9210 [==============================] - 17s 2ms/step - loss: 2.2017 - acc: 0.3545\n",
      "Epoch 6/30\n",
      "9210/9210 [==============================] - 17s 2ms/step - loss: 2.1356 - acc: 0.3744\n",
      "Epoch 7/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 2.0693 - acc: 0.3914\n",
      "Epoch 8/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 2.0039 - acc: 0.4037\n",
      "Epoch 9/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 1.9450 - acc: 0.4156\n",
      "Epoch 10/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 1.8811 - acc: 0.4305\n",
      "Epoch 11/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 1.8139 - acc: 0.4516\n",
      "Epoch 12/30\n",
      "9210/9210 [==============================] - 20s 2ms/step - loss: 1.7382 - acc: 0.4726\n",
      "Epoch 13/30\n",
      "9210/9210 [==============================] - 20s 2ms/step - loss: 1.6536 - acc: 0.4965\n",
      "Epoch 14/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 1.5588 - acc: 0.5251\n",
      "Epoch 15/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 1.4525 - acc: 0.5558\n",
      "Epoch 16/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 1.3345 - acc: 0.5934\n",
      "Epoch 17/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 1.1996 - acc: 0.6433\n",
      "Epoch 18/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 1.0628 - acc: 0.6873\n",
      "Epoch 19/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.9235 - acc: 0.7352\n",
      "Epoch 20/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.7919 - acc: 0.7832\n",
      "Epoch 21/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.6570 - acc: 0.8341\n",
      "Epoch 22/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.5464 - acc: 0.8691\n",
      "Epoch 23/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.4440 - acc: 0.9076\n",
      "Epoch 24/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.3598 - acc: 0.9357\n",
      "Epoch 25/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.2873 - acc: 0.9555\n",
      "Epoch 26/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.2373 - acc: 0.9673\n",
      "Epoch 27/30\n",
      "9210/9210 [==============================] - 18s 2ms/step - loss: 0.1876 - acc: 0.9809\n",
      "Epoch 28/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 0.1485 - acc: 0.9879\n",
      "Epoch 29/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 0.1050 - acc: 0.9955\n",
      "Epoch 30/30\n",
      "9210/9210 [==============================] - 19s 2ms/step - loss: 0.0914 - acc: 0.9963\n"
     ]
    }
   ],
   "source": [
    "train_model(10, 200, 30, 'test_model_2.h')\n",
    "#train_model(15, 250, 25, 'hip_hop2.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by (but not copied from) the keras example Github\n",
    "def sample(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    e_predictions = np.exp(predictions)\n",
    "    predictions = e_predictions / np.sum(e_predictions)\n",
    "    probs = np.random.multinomial(1, predictions[0], 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def generate_text(model_filename, seed_text, n_chars, temp):\n",
    "    model = load_model(model_filename)\n",
    "    for _ in range(n_chars):\n",
    "        temp_seed = seed_text[-40:]\n",
    "        encoded = [mapping_c2i[char] for char in temp_seed]\n",
    "        encoded = np.array(to_categorical(encoded, num_classes=len(mapping_c2i)))\n",
    "        encoded = np.reshape(encoded, (1, encoded.shape[0], encoded.shape[1]))\n",
    "\n",
    "        preds = model.predict(encoded, verbose=0)\n",
    "        ind = sample(preds, temperature=temp)\n",
    "        seed_text += mapping_i2c[ind]\n",
    "    return seed_text\n",
    "\n",
    "def generate_poem(model_filename, seed_text, temp):\n",
    "    poem_text = generate_text(model_filename, seed_text, 520, temp)\n",
    "    lines = [poem_text[i:i+40] for i in range(0, 520, 40)]\n",
    "    poem = ''\n",
    "    for line in lines:\n",
    "        if '\\n' in line:\n",
    "            line = line.replace('\\n', '')\n",
    "        poem += line\n",
    "        poem += '\\n'\n",
    "    return poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem with 1.5 temperature\n",
      "shall i compare thee to a summer's day?\n",
      "owexfadmowh tholst fen and me,beit ghatt\n",
      " doth minds wpei hrave doon hase,whish i\n",
      " jove that sichtwer breentulnenswed care\n",
      "chidg tongst fy love frevevt:whtt tine i\n",
      "n knyer qaies bo thince siccadgich offrr\n",
      "were,se, and mush chagh dus and cinesent\n",
      "ss on beains,ho gavtszo yos rrss.isweje \n",
      "bo on ers noued so my trees from ase mos\n",
      "trrss wrensh bid,s by thy owhe, ast to t\n",
      "he'r pruistrese,what basbead shoull tome\n",
      " yousale batiestinghdas ors flifere butk\n",
      "shtseichadsed d chack laks goed com,thic\n",
      "\n",
      "Poem with 0.75 temperature\n",
      "shall i compare thee to a summer's day?\n",
      "y ailftr owres chouts thouss swrets doos\n",
      "s wrogh dut in sher, on whish whrt in do\n",
      "st tongadds llasing of al nore's with su\n",
      "alf love's batt hing in that reatung tha\n",
      "t looss songs fross then shate ofirgore'\n",
      "s brensere nat hase but of my lever no s\n",
      "hat pome,fofloth nat inqusen ene,d seefe\n",
      "nghy siuld beady hot hish dorg thou sove\n",
      " foof faald bo holr wnot hasco sires ond\n",
      " for in,and fointing hare chaighther for\n",
      "es, thet hinguge'ss and wruchougit: bat \n",
      "but il shomer love swellebthere buth thu\n",
      "\n",
      "Poem with 0.25 temperature\n",
      "shall i compare thee to a summer's day?\n",
      "o and remmore shath, trees co bodt wowh \n",
      "though the sing,ard by thar maste me fee\n",
      "prom heed but asfir with thy carse seall\n",
      "st stlens seas but is prome therss the s\n",
      "ondy sare.to bease art remplice sulle,yo\n",
      "t list to stare,a dover by the mecress m\n",
      "o fort,wo love that share stuen,ard in h\n",
      "opr dowes steel sthan i sucht love sunde\n",
      "rsain, of thot not hish desir gove, no  \n",
      "hat i pucce say love, woth that whe him \n",
      "dess to se.wot do bate o smert sweer sta\n",
      "ye, sills whor thou move what frimm forl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "print(\"Poem with 1.5 temperature\")\n",
    "print(generate_poem('test_model_2.h', \"shall i compare thee to a summer's day?\\n\", 1.5))\n",
    "\n",
    "print(\"Poem with 0.75 temperature\")\n",
    "print(generate_poem('test_model_2.h', \"shall i compare thee to a summer's day?\\n\", 0.75))\n",
    "\n",
    "print(\"Poem with 0.25 temperature\")\n",
    "print(generate_poem('test_model_2.h', \"shall i compare thee to a summer's day?\\n\", 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem with 0.15 temperature\n",
      "shall i compare thee to a summer's day?\n",
      "y aillt, that sthen the bucuiferis bady \n",
      "ar my reasurgase and and,dos best it sim\n",
      "f,  for in entir fainothat i const ferer\n",
      " wowhate,that thou not praine,what is my\n",
      " soll no saire,whs io thy heart thee my \n",
      "reairwhat sumend sight be bute si love, \n",
      "now hours sate aich of floode,that sicht\n",
      " fis love's batt his hien, sight is bain\n",
      "y bate,a dosh mend, rwhimet thy sey wath\n",
      " stulle's neded mo badt pom my fove that\n",
      " suclove stolle, with hou love not sairs\n",
      " of my wrich sight for love, hot hish ba\n",
      "\n",
      "Poem with 0.1 temperature\n",
      "shall i compare thee to a summer's day?\n",
      "y aillt, that sthen the bucuie, o have i\n",
      "n deersto say woth my love so love, wnst\n",
      " bost if thas beat thy shad,a tuee me lo\n",
      "ve youl nith, simmor that wire shiin eys\n",
      " and doth fishad i has pare,and fair the\n",
      " comerdst thy swail, you govet,  sway wh\n",
      "e love to sout wot stares of not prom th\n",
      "en braine,than i succt love sueles:by th\n",
      "ou com't to se.wrich shith pyrist dosh t\n",
      "hress thou swiet, stome's batt to siepi \n",
      "goty a iow, and fooling that succuse in \n",
      "sear bat eys all goods bet bat how om my\n",
      "\n",
      "Poem with 0.05 temperature\n",
      "shall i compare thee to a summer's day?\n",
      "y aillt, that sthen the bucuige,with sha\n",
      "t my pain,and 'sing nee in of seawest sa\n",
      "ll dringus beaughth ou rece she love,who\n",
      "t but as fis love sunders ar shate, aid \n",
      "thee, ith is mand owh livest thou givet \n",
      "thy love saal nhad,  that not past i fre\n",
      "are,and is plaise i sair what is hall po\n",
      "rty saild bus one, fill thou simer and b\n",
      "at in has ower, and thou not in ancentin\n",
      "g seed.a doass are you, with that not pa\n",
      "st are laies of my poment, nat sthe dead\n",
      "ing doth lise tish pinct lase of spart f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Poem with 0.15 temperature\")\n",
    "print(generate_poem('test_model_2.h', \"shall i compare thee to a summer's day?\\n\", 0.15))\n",
    "\n",
    "print(\"Poem with 0.1 temperature\")\n",
    "print(generate_poem('test_model_2.h', \"shall i compare thee to a summer's day?\\n\", 0.1))\n",
    "\n",
    "print(\"Poem with 0.05 temperature\")\n",
    "print(generate_poem('test_model_2.h', \"shall i compare thee to a summer's day?\\n\", 0.05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
