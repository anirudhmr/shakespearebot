{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "def gen_rnn_data(n):\n",
    "    all_text = \"\"\n",
    "    with open('../data/shakespeare.txt') as inp:\n",
    "        for line in inp:\n",
    "            all_text += line[:-1]\n",
    "    seqs = []\n",
    "    for i in range(40, len(all_text), n):\n",
    "        seqs.append(all_text[i-40:i+1])\n",
    "    return seqs  \n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')s\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "lines1 = gen_rnn_data(10)\n",
    "raw_text1 = '\\n'.join(lines1)\n",
    "\n",
    "# Creating an encoding for each encountered character\n",
    "chars1 = sorted(list(set(raw_text1)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars1))\n",
    "def train_model(seq_skip, n_nodes, n_epochs, model_filename):\n",
    "    lines = gen_rnn_data(seq_skip)\n",
    "    raw_text = '\\n'.join(lines)\n",
    "\n",
    "    # Creating an encoding for each encountered character\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "    seqs = list()\n",
    "    for line in lines:\n",
    "        # Replace string with encoding of each character\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        # Replace list of sequences with list of encoded sequences\n",
    "        seqs.append(encoded_seq)\n",
    "\n",
    "    # Number of distinct characters\n",
    "    vocab_size = len(mapping)\n",
    "\n",
    "    # Splitting into input and output\n",
    "    seqs = np.array(seqs)\n",
    "    X, y = seqs[:, :-1], seqs[:, -1]\n",
    "\n",
    "    # one-hot encoding\n",
    "    seqs = [to_categorical(x, num_classes = vocab_size) for x in X]\n",
    "    X = np.array(seqs)\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "    # Model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_nodes, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=n_epochs, verbose=1)\n",
    "\n",
    "    model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "9538/9538 [==============================] - 23s 2ms/step - loss: 3.0537 - acc: 0.1980\n",
      "Epoch 2/30\n",
      "9538/9538 [==============================] - 18s 2ms/step - loss: 2.7846 - acc: 0.2534\n",
      "Epoch 3/30\n",
      "9538/9538 [==============================] - 18s 2ms/step - loss: 2.5005 - acc: 0.3157\n",
      "Epoch 4/30\n",
      "9538/9538 [==============================] - 18s 2ms/step - loss: 2.4603 - acc: 0.3164\n",
      "Epoch 5/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 2.3204 - acc: 0.3476\n",
      "Epoch 6/30\n",
      "9538/9538 [==============================] - 18s 2ms/step - loss: 2.2391 - acc: 0.3706\n",
      "Epoch 7/30\n",
      "9538/9538 [==============================] - 25s 3ms/step - loss: 2.1797 - acc: 0.3803\n",
      "Epoch 8/30\n",
      "9538/9538 [==============================] - 25s 3ms/step - loss: 2.1345 - acc: 0.3902\n",
      "Epoch 9/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 2.0936 - acc: 0.3984\n",
      "Epoch 10/30\n",
      "9538/9538 [==============================] - 20s 2ms/step - loss: 2.0589 - acc: 0.4094\n",
      "Epoch 11/30\n",
      "9538/9538 [==============================] - 18s 2ms/step - loss: 2.0241 - acc: 0.4122\n",
      "Epoch 12/30\n",
      "9538/9538 [==============================] - 18s 2ms/step - loss: 1.9919 - acc: 0.4175\n",
      "Epoch 13/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.9639 - acc: 0.4292\n",
      "Epoch 14/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.9335 - acc: 0.4302\n",
      "Epoch 15/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.9001 - acc: 0.4376\n",
      "Epoch 16/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.8700 - acc: 0.4494\n",
      "Epoch 17/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.8356 - acc: 0.4558\n",
      "Epoch 18/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.8045 - acc: 0.4657\n",
      "Epoch 19/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.7706 - acc: 0.4710\n",
      "Epoch 20/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.7443 - acc: 0.4821\n",
      "Epoch 21/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.7042 - acc: 0.4900\n",
      "Epoch 22/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.7378 - acc: 0.4818\n",
      "Epoch 23/30\n",
      "9538/9538 [==============================] - 20s 2ms/step - loss: 1.6585 - acc: 0.5039\n",
      "Epoch 24/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.6153 - acc: 0.5165\n",
      "Epoch 25/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.5869 - acc: 0.5239\n",
      "Epoch 26/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.5490 - acc: 0.5360\n",
      "Epoch 27/30\n",
      "9538/9538 [==============================] - 20s 2ms/step - loss: 1.5009 - acc: 0.5500\n",
      "Epoch 28/30\n",
      "9538/9538 [==============================] - 19s 2ms/step - loss: 1.4593 - acc: 0.5642\n",
      "Epoch 29/30\n",
      "9538/9538 [==============================] - 20s 2ms/step - loss: 1.4237 - acc: 0.5675\n",
      "Epoch 30/30\n",
      "9538/9538 [==============================] - 21s 2ms/step - loss: 1.3764 - acc: 0.5853\n"
     ]
    }
   ],
   "source": [
    "train_model(10, 200, 30, 'test_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model_filename, seed_text, n_chars):\n",
    "    model = load_model(model_filename)\n",
    "    for _ in range(n_chars):\n",
    "        temp_seed = seed_text[-40:]\n",
    "        encoded = [mapping[char] for char in temp_seed]\n",
    "        encoded = array(to_categorical(encoded, num_classes=len(mapping)))\n",
    "        encoded = np.reshape(encoded, (1, encoded.shape[0], encoded.shape[1]))\n",
    "\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        seed_text += char\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From fairest creatures we desire increasend of thee mare the prome thee mane my leave '"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('test_model_2.h5', 'From fairest creatures we desire increase', 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, ';': 20, '?': 21, 'A': 22, 'B': 23, 'C': 24, 'D': 25, 'E': 26, 'F': 27, 'G': 28, 'H': 29, 'I': 30, 'J': 31, 'K': 32, 'L': 33, 'M': 34, 'N': 35, 'O': 36, 'P': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'V': 42, 'W': 43, 'Y': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'w': 67, 'x': 68, 'y': 69, 'z': 70}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
