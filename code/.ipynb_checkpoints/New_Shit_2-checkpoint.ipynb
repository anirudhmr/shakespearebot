{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'X': 46, 'Y': 47, '[': 48, ']': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, 'ś': 77, '’': 78, '…': 79}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "# From the keras lstm_text_generation.py Github example\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "def gen_rnn_data(n, filename):\n",
    "    all_text = \"\"\n",
    "    with open(filename) as inp:\n",
    "        for line in inp:\n",
    "            all_text += line[:-1]\n",
    "    seqs = []\n",
    "    for i in range(40, len(all_text), n):\n",
    "        seqs.append(all_text[i-40:i+1])\n",
    "    chars = sorted(list(set('\\n'.join(seqs))))\n",
    "    char_2_index = dict((c, i) for i, c in enumerate(chars))\n",
    "    index_2_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    return seqs, char_2_index, index_2_char \n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "_, mapping_c2i, mapping_i2c = gen_rnn_data(10, '../data/ks.txt')\n",
    "print(mapping_c2i)\n",
    "\n",
    "def train_model(seq_skip, n_nodes, n_epochs, model_filename):\n",
    "    lines, mapping_c2i, mapping_i2c = gen_rnn_data(seq_skip, '../data/ks.txt')\n",
    "    all_text = '\\n'.join(lines)\n",
    "\n",
    "    seqs = []\n",
    "    for line in lines:\n",
    "        # Replace string with encoding of each character\n",
    "        encoded_seq = [mapping_c2i[char] for char in line]\n",
    "        # Replace list of sequences with list of encoded sequences\n",
    "        seqs.append(encoded_seq)\n",
    "\n",
    "    # Number of distinct characters\n",
    "    num_chars = len(mapping_c2i)\n",
    "    print(num_chars)\n",
    "    # Splitting into input strings and output character\n",
    "    seqs = np.array(seqs)\n",
    "    X, y = seqs[:, :-1], seqs[:, -1]\n",
    "\n",
    "    # one-hot encoding\n",
    "    seqs = [to_categorical(x, num_classes = num_chars) for x in X]\n",
    "    X = np.array(seqs)\n",
    "    y = to_categorical(y, num_classes=num_chars)\n",
    "\n",
    "    # Model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_nodes, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dense(num_chars, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=n_epochs, verbose=1)\n",
    "\n",
    "    model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 250)               331000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 80)                20080     \n",
      "=================================================================\n",
      "Total params: 351,080\n",
      "Trainable params: 351,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "2540/2540 [==============================] - 13s 5ms/step - loss: 3.2841 - acc: 0.1858\n",
      "Epoch 2/25\n",
      "2540/2540 [==============================] - 7s 3ms/step - loss: 3.0905 - acc: 0.1898\n",
      "Epoch 3/25\n",
      "2540/2540 [==============================] - 8s 3ms/step - loss: 3.0689 - acc: 0.1925\n",
      "Epoch 4/25\n",
      "2540/2540 [==============================] - 7s 3ms/step - loss: 3.0397 - acc: 0.1898\n",
      "Epoch 5/25\n",
      "2540/2540 [==============================] - 9s 4ms/step - loss: 2.9784 - acc: 0.1988\n",
      "Epoch 6/25\n",
      "2540/2540 [==============================] - 10s 4ms/step - loss: 2.8864 - acc: 0.2291\n",
      "Epoch 7/25\n",
      "2540/2540 [==============================] - 10s 4ms/step - loss: 2.7878 - acc: 0.2717\n",
      "Epoch 8/25\n",
      "2540/2540 [==============================] - 10s 4ms/step - loss: 2.6930 - acc: 0.2961\n",
      "Epoch 9/25\n",
      "2540/2540 [==============================] - 11s 4ms/step - loss: 2.6169 - acc: 0.3016\n",
      "Epoch 10/25\n",
      "2540/2540 [==============================] - 9s 4ms/step - loss: 2.5494 - acc: 0.3094\n",
      "Epoch 11/25\n",
      "2540/2540 [==============================] - 9s 4ms/step - loss: 2.4832 - acc: 0.3327\n",
      "Epoch 12/25\n",
      "2540/2540 [==============================] - 8s 3ms/step - loss: 2.4277 - acc: 0.3394\n",
      "Epoch 13/25\n",
      "2540/2540 [==============================] - 9s 3ms/step - loss: 2.3624 - acc: 0.3504\n",
      "Epoch 14/25\n",
      "2540/2540 [==============================] - 9s 3ms/step - loss: 2.3166 - acc: 0.3551\n",
      "Epoch 15/25\n",
      "2540/2540 [==============================] - 10s 4ms/step - loss: 2.2666 - acc: 0.3740\n",
      "Epoch 16/25\n",
      "2540/2540 [==============================] - 11s 4ms/step - loss: 2.2005 - acc: 0.3846\n",
      "Epoch 17/25\n",
      "2540/2540 [==============================] - 10s 4ms/step - loss: 2.1627 - acc: 0.3953\n",
      "Epoch 18/25\n",
      "2540/2540 [==============================] - 8s 3ms/step - loss: 2.1098 - acc: 0.4067\n",
      "Epoch 19/25\n",
      "2540/2540 [==============================] - 10s 4ms/step - loss: 2.0554 - acc: 0.4209\n",
      "Epoch 20/25\n",
      "2540/2540 [==============================] - 8s 3ms/step - loss: 2.0043 - acc: 0.4220\n",
      "Epoch 21/25\n",
      "2540/2540 [==============================] - 8s 3ms/step - loss: 1.9461 - acc: 0.4358\n",
      "Epoch 22/25\n",
      "2540/2540 [==============================] - 8s 3ms/step - loss: 1.8876 - acc: 0.4583\n",
      "Epoch 23/25\n",
      "2540/2540 [==============================] - 10s 4ms/step - loss: 1.8444 - acc: 0.4614\n",
      "Epoch 24/25\n",
      "2540/2540 [==============================] - 11s 4ms/step - loss: 1.7871 - acc: 0.4697: 0s - loss: 1.7859 - acc: 0.4\n",
      "Epoch 25/25\n",
      "2540/2540 [==============================] - 11s 4ms/step - loss: 1.7152 - acc: 0.4965\n"
     ]
    }
   ],
   "source": [
    "#train_model(10, 200, 30, 'test_model_2.h')\n",
    "train_model(15, 250, 25, 'hip_hop2.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by (but not copied from) the keras example Github\n",
    "def sample(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    e_predictions = np.exp(predictions)\n",
    "    predictions = e_predictions / np.sum(e_predictions)\n",
    "    probs = np.random.multinomial(1, predictions[0], 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def generate_text(model_filename, seed_text, n_chars, temp):\n",
    "    model = load_model(model_filename)\n",
    "    for _ in range(n_chars):\n",
    "        temp_seed = seed_text[-40:]\n",
    "        encoded = [mapping_c2i[char] for char in temp_seed]\n",
    "        encoded = np.array(to_categorical(encoded, num_classes=len(mapping_c2i)))\n",
    "        encoded = np.reshape(encoded, (1, encoded.shape[0], encoded.shape[1]))\n",
    "\n",
    "        preds = model.predict(encoded, verbose=0)\n",
    "        ind = sample(preds, temperature=temp)\n",
    "        seed_text += mapping_i2c[ind]\n",
    "        \n",
    "    \n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus is his cheek the map of days outwor whe heactool berser er sore berker to me thee to llace,The merseate the  to  ou the heact ou dol' tore, the  the  tre  or beaceane heacero ghe buree  o  live ghe auplline hou dol' seace to d al ho deate he seane herstore the are,  on buthe, the  ou llise the mursein the houd bo meace he stone he seace be she bursein'  pa beede bous buthen the  ou lllserte tou des the  al ho ghe gou lous the he soun the he seave deact ou stoll the dese buthee tore to the gree, ou sease, the serseer beater mive hous the he, hou seate to d as the herstore berker mive he seace, the  an the he seane hou don's the  in the  to live heastore hous the herser the  the  tone the herser the  t ol  ind in the he store, the he soone the de, the  tr live beaster the hersthe dease tool bedseat the he seave he store tore tool be beane he core, on whe he deace toll to live he seane he seave, done hous the or beaceane hore,  on the herstore gone seane heace ho shote hou seace to lite bo dous bith to s in the he stone he\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "print(generate_text('hip_hop2.h', \"Thus is his cheek the map of days outwor\", 1000, 0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, ';': 20, '?': 21, 'A': 22, 'B': 23, 'C': 24, 'D': 25, 'E': 26, 'F': 27, 'G': 28, 'H': 29, 'I': 30, 'J': 31, 'K': 32, 'L': 33, 'M': 34, 'N': 35, 'O': 36, 'P': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'V': 42, 'W': 43, 'Y': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'w': 67, 'x': 68, 'y': 69, 'z': 70}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rnn_data(n, filename, hot_size):\n",
    "    all_text = \"\"\n",
    "    with open(filename) as inp:\n",
    "        for line in inp:\n",
    "            all_text += line[:-1]\n",
    "    words = all_text.lower().split()\n",
    "    for i in range(5, len(words), n):\n",
    "        seqs.append(words[i-5:i+1])\n",
    "    encoded_words = [one_hot(d, hot_size) for d in words]\n",
    "    return encoded_words\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "_, mapping_c2i, mapping_i2c = gen_rnn_data(10, '../data/ks.txt')\n",
    "print(mapping_c2i)\n",
    "\n",
    "def train_model(seq_skip, n_nodes, n_epochs, model_filename):\n",
    "    lines, mapping_c2i, mapping_i2c = gen_rnn_data(seq_skip, '../data/ks.txt')\n",
    "    all_text = '\\n'.join(lines)\n",
    "\n",
    "    seqs = []\n",
    "    for line in lines:\n",
    "        # Replace string with encoding of each character\n",
    "        encoded_seq = [mapping_c2i[char] for char in line]\n",
    "        # Replace list of sequences with list of encoded sequences\n",
    "        seqs.append(encoded_seq)\n",
    "\n",
    "    # Number of distinct characters\n",
    "    num_chars = len(mapping_c2i)\n",
    "    print(num_chars)\n",
    "    # Splitting into input strings and output character\n",
    "    seqs = np.array(seqs)\n",
    "    X, y = seqs[:, :-1], seqs[:, -1]\n",
    "\n",
    "    # one-hot encoding\n",
    "    seqs = [to_categorical(x, num_classes = num_chars) for x in X]\n",
    "    X = np.array(seqs)\n",
    "    y = to_categorical(y, num_classes=num_chars)\n",
    "\n",
    "    # Model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_nodes, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dense(num_chars, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=n_epochs, verbose=1)\n",
    "\n",
    "    model.save(model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
